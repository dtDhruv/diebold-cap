{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Test Automation with BLIP-2\n",
    "\n",
    "This notebook trains a multimodal model to generate GUI test steps from screenshots and functional descriptions.\n",
    "\n",
    "**Architecture:** BLIP-2 (Vision Encoder + Q-Former + FLAN-T5)\n",
    "\n",
    "**Dataset:** SuperAGI/GUIDE\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup & Installation\n",
    "2. Load Dataset\n",
    "3. Initialize Model\n",
    "4. Training\n",
    "5. Evaluation\n",
    "6. Inference Demo\n",
    "7. Sequence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision transformers datasets pillow accelerate bitsandbytes\n",
    "!pip install -q rouge-score nltk wandb gradio evaluate\n",
    "!pip install -q -U transformers  # Ensure latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already in Colab)\n",
    "import os\n",
    "if not os.path.exists('diebold-cap'):\n",
    "    !git clone YOUR_REPO_URL diebold-cap\n",
    "    \n",
    "%cd diebold-cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from data.dataset import GUITestDataset, get_dataloaders\n",
    "from models.blip2_model import GUITestBLIP2\n",
    "from training.trainer import GUITestTrainer\n",
    "from utils.evaluation import GUITestEvaluator, evaluate_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Loading the SuperAGI/GUIDE dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to explore\n",
    "dataset = GUITestDataset(split='train', max_history_length=10)\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "\n",
    "# Show example\n",
    "sample = dataset[0]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example Sample\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nWorkflow: {sample['workflow']}\")\n",
    "print(f\"\\nInput Text:\\n{sample['input_text']}\")\n",
    "print(f\"\\nTarget Text: {sample['target_text']}\")\n",
    "print(\"\\nImage:\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis('off')\n",
    "plt.title(f\"Screenshot - {sample['workflow']}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show more examples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    sample = dataset[i * 100]  # Sample every 100th\n",
    "    axes[i].imshow(sample['image'])\n",
    "    axes[i].set_title(f\"Action: {sample['target_text'][:50]}...\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model\n",
    "\n",
    "Loading BLIP-2 with FLAN-T5-base backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GUITestBLIP2(\n",
    "    model_name=\"Salesforce/blip2-flan-t5-base\",\n",
    "    device=device,\n",
    "    freeze_vision=True,      # Keep vision encoder frozen (saves memory)\n",
    "    freeze_qformer=False,     # Train Q-Former\n",
    "    freeze_lm_encoder=True,   # Freeze T5 encoder, train decoder\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with sample\n",
    "print(\"Testing model generation...\\n\")\n",
    "sample = dataset[0]\n",
    "prediction = model.generate(\n",
    "    images=[sample['image']],\n",
    "    prompts=[sample['input_text']],\n",
    "    max_length=50,\n",
    "    num_beams=2\n",
    ")\n",
    "\n",
    "print(f\"Input: {sample['input_text'][:200]}...\")\n",
    "print(f\"\\nPredicted action: {prediction[0]}\")\n",
    "print(f\"Ground truth: {sample['target_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Training with memory-efficient settings for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    batch_size=4,  # Small batch for memory efficiency\n",
    "    num_workers=2,\n",
    "    processor=model.processor,\n",
    "    device=device,\n",
    "    max_history_length=10,\n",
    "    use_cot=False  # Don't use chain-of-thought initially\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 3,\n",
    "    'gradient_accumulation_steps': 4,  # Effective batch = 4 * 4 = 16\n",
    "    'max_grad_norm': 1.0,\n",
    "    'warmup_steps': 100,\n",
    "    'use_wandb': False,  # Set True if you want WandB logging\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = GUITestTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    output_dir='./outputs',\n",
    "    **training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate the trained model on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = GUITestBLIP2(device=device)\n",
    "best_model.load_model('./outputs/best_model')\n",
    "print(\"Best model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "evaluator = GUITestEvaluator()\n",
    "results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader,\n",
    "    evaluator=evaluator,\n",
    "    max_batches=50  # Evaluate on subset for speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "val_dataset = GUITestDataset(split='validation')\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "\n",
    "for i in range(3):\n",
    "    sample_idx = i * 50\n",
    "    sample = val_dataset[sample_idx]\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = best_model.generate(\n",
    "        images=[sample['image']],\n",
    "        prompts=[sample['input_text']],\n",
    "        max_length=128,\n",
    "        num_beams=4\n",
    "    )[0]\n",
    "    \n",
    "    # Display image\n",
    "    axes[i, 0].imshow(sample['image'])\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].set_title(f\"Screenshot {i+1}\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Display text\n",
    "    text_content = f\"\"\"Task: {sample['raw_data']['question'][:100]}...\n",
    "    \n",
    "PREDICTED:\n",
    "{prediction}\n",
    "\n",
    "GROUND TRUTH:\n",
    "{sample['target_text']}\n",
    "\n",
    "MATCH: {prediction.lower() == sample['target_text'].lower()}\n",
    "\"\"\"\n",
    "    axes[i, 1].text(0.05, 0.95, text_content, \n",
    "                    transform=axes[i, 1].transAxes,\n",
    "                    fontsize=10, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Demo\n",
    "\n",
    "Test the model interactively with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict_next_action(image, task, history_text, current_action):\n",
    "    \"\"\"\n",
    "    Predict next action given screenshot and context.\n",
    "    \"\"\"\n",
    "    # Format input\n",
    "    if not history_text.strip():\n",
    "        history_text = \"None\"\n",
    "    if not current_action.strip():\n",
    "        current_action = \"None\"\n",
    "    \n",
    "    prompt = f\"\"\"Task: {task}\n",
    "\n",
    "Previous steps:\n",
    "{history_text}\n",
    "\n",
    "Current action: {current_action}\n",
    "\n",
    "Predict the next action:\"\"\"\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = best_model.generate(\n",
    "        images=[image],\n",
    "        prompts=[prompt],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        temperature=0.7\n",
    "    )[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_next_action,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Screenshot\"),\n",
    "        gr.Textbox(label=\"Task Description\", placeholder=\"e.g., Login to the application\"),\n",
    "        gr.Textbox(label=\"Previous Steps (one per line)\", lines=5, placeholder=\"1. Open homepage\\n2. Click login\"),\n",
    "        gr.Textbox(label=\"Current Action\", placeholder=\"e.g., Click login button\"),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Predicted Next Action\"),\n",
    "    title=\"GUI Test Automation - Next Step Predictor\",\n",
    "    description=\"Upload a screenshot and provide context to predict the next test step.\",\n",
    "    examples=[\n",
    "        # You can add example images here\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequence Generation\n",
    "\n",
    "Generate full test sequences autoregressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full sequence\n",
    "sample = val_dataset[10]\n",
    "task = sample['raw_data']['question']\n",
    "\n",
    "print(f\"Task: {task}\\n\")\n",
    "print(\"Generating test sequence...\\n\")\n",
    "\n",
    "sequence = best_model.generate_sequence(\n",
    "    initial_image=sample['image'],\n",
    "    question=task,\n",
    "    max_steps=10,\n",
    "    max_length=128,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Generated Test Sequence\")\n",
    "print(\"=\"*60)\n",
    "for step in sequence:\n",
    "    print(f\"\\nStep {step['step_num']}: {step['action']}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequence\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis('off')\n",
    "plt.title(f\"Task: {task}\\n\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text box with sequence\n",
    "sequence_text = \"\\n\".join([f\"{s['step_num']}. {s['action']}\" for s in sequence])\n",
    "plt.text(0.5, -0.15, f\"Generated Sequence:\\n{sequence_text}\",\n",
    "         ha='center', va='top', transform=plt.gca().transAxes,\n",
    "         fontsize=11, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "Save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive (if mounted)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save model\n",
    "best_model.save_model('/content/drive/MyDrive/gui_test_automation_model')\n",
    "print(\"✓ Model saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading GUIDE dataset\n",
    "2. ✅ Fine-tuning BLIP-2 for GUI test automation\n",
    "3. ✅ Evaluating with multiple metrics (BLEU, ROUGE, EM)\n",
    "4. ✅ Interactive demo with Gradio\n",
    "5. ✅ Autoregressive sequence generation\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with different hyperparameters\n",
    "- Try larger models (BLIP-2 with T5-XL)\n",
    "- Add chain-of-thought reasoning\n",
    "- Fine-tune on domain-specific data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
